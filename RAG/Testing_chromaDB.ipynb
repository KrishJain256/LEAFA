{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/run/media/aasiku/Vault/GlobalPythonEnv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_3567/4230958642.py:13: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(embedding_function=embeddings, persist_directory=\"db_directory\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "# Initialize the embeddings\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "GEMINI_API_KEY = os.environ.get(\"GEMINI_API\")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\",google_api_key=GEMINI_API_KEY)\n",
    "# Create a Chroma instance\n",
    "vector_store = Chroma(embedding_function=embeddings, persist_directory=\"db_directory\")\n",
    "\n",
    "# Load documents (replace with your own document loading logic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = [[Document(metadata={}, page_content=\"File_path: Papers/The Power of Linear Recurrent Neural Networks.pdf \\nThis document delves into the capabilities of Linear Recurrent Neural Networks (LRNNs) for time series analysis. LRNNs, unlike traditional RNNs, utilize linear activation functions, resulting in efficient learning and compact network sizes. The paper presents a closed-form training method for LRNNs, involving linear regression for output weight learning and eigenvalue analysis for network size reduction. LRNNs possess properties such as an approximation theorem, predictable dynamics (often converging to ellipse trajectories), and dimensionality reduction through eigenvalue analysis. Experimental results on multiple superimposed oscillators, number puzzles, robot soccer simulation, and stock price prediction demonstrate LRNNs' effectiveness in capturing complex temporal dependencies and achieving high prediction accuracy. The document concludes that LRNNs offer a potent and efficient approach for time series analysis, with potential applications in neuromorphic computing, reservoir computing, and other machine learning tasks.\\n\\nKeywords: Linear Recurrent Neural Networks (LRNNs), Time Series Analysis, Prediction, Approximation Theorem, Dimensionality Reduction, Network Size Reduction, Ellipse Trajectories, Eigenvalues, Transition Matrix, Linear Activation, Learning Algorithm, Multiple Superimposed Oscillators (MSO), Number Puzzles, Robot Soccer Simulation, Stock Price Prediction, Neuromorphic Computing, Reservoir Computing. \\n\")],\n",
    " [Document(metadata={}, page_content='File_path: Papers/Attention_is_all_you_need.pdf \\n## Summary:\\n\\nThe Transformer is a revolutionary neural network architecture designed for sequence transduction tasks, particularly machine translation, surpassing traditional RNNs in both performance and efficiency. This groundbreaking model utilizes self-attention, eliminating the need for recurrent connections and convolutions, enabling faster training and parallel processing. Key features include an encoder-decoder structure, multi-head attention for capturing long-range dependencies, and positional encodings to preserve token order. The Transformer achieved state-of-the-art results on English-to-German and English-to-French translation tasks, outperforming existing models, including ensembles, in both translation quality and training speed. It also demonstrated competitive performance on English constituency parsing. Future research aims to extend the Transformer to other tasks, explore different modalities, and investigate local attention mechanisms for handling large inputs and outputs.\\n\\n## Keywords:\\n\\nTransformer, Neural Network, Sequence Transduction, Machine Translation, Self-Attention, Multi-Head Attention, Positional Encoding, Encoder-Decoder, RNN, Convolutional Layer, Parallelism, Computational Efficiency, Long-Range Dependencies, WMT 2014, English-to-German, English-to-French, Constituency Parsing, State-of-the-Art, BLEU Score, Training Speed \\n')],\n",
    " [Document(metadata={}, page_content='File_path: Papers/StackGAN.pdf \\n## Summary:\\n\\nStackGAN is a generative adversarial network (GAN) that excels at generating high-resolution images from textual descriptions. It uses a text encoder to convert both text and images into a shared feature space. The process is divided into two stages: Stage-I generates low-resolution images capturing basic shapes and colors, while Stage-II refines these images, adding details and correcting imperfections. Both stages employ objective functions for the discriminator and generator, aiming to minimize the difference between real and generated images. The training process involves optimizing these functions using backpropagation. \\n\\n## Keywords: \\n\\nStackGAN, GAN, Text-to-Image Synthesis, Image Generation, Text Embedding, Gaussian Conditioning, Reparameterization Trick, Text Encoder, Residual Blocks, Matching-Aware Discriminator, Low-resolution Image, High-resolution Image, Stage-I, Stage-II, Discriminator, Generator, Objective Functions, Training Process. \\n')],\n",
    " [Document(metadata={}, page_content='File_path: Papers/Image_Augmentation_IllusionCraft.pdf \\n## Summary:\\n\\nThis document delves into the crucial role of data augmentation in computer vision, particularly for deep learning models. It tackles key challenges in computer vision, including image variations, class imbalance, domain shift, and overfitting. The document explores both traditional and cutting-edge image augmentation techniques. Classical techniques, such as flipping, rotating, cropping, resizing, color jittering, adding noise, warping, and random erasing, are thoroughly explained. Advanced techniques like Cutout, Mixup, Cutmix, and Augmix are discussed, providing illustrations, advantages, limitations, and hyperparameter considerations for each. The document emphasizes that data augmentation significantly enhances model performance by increasing data diversity and mitigating overfitting. It concludes by highlighting the benefits of advanced techniques in generating more diverse and realistic training data, leading to more robust feature learning. \\n\\n## Keywords:\\n\\nData Augmentation, Computer Vision, Deep Learning, Image Variations, Class Imbalance, Domain Shift, Overfitting, Classical Techniques, Advanced Techniques, Flipping, Rotating, Cropping, Resizing, Color Jittering, Adding Noise, Warping, Random Erasing, Cutout, Mixup, Cutmix, Augmix, Model Performance, Robustness, Feature Learning. \\n')],\n",
    " [Document(metadata={}, page_content=\"File_path: Papers/Mismatching_images___Keeping_a_check_on_the_generator (1).pdf \\nThis document explores a technique for improving the image-text alignment capabilities of StackGAN models by introducing intentionally mismatched image-text pairs during training. The discriminator learns to identify both real/fake images and correctly/incorrectly aligned image-text pairs by being exposed to these mismatched pairs. This is achieved by shifting the image batch and pairing an image with the text embedding of the next image in the batch. This exposure to negative examples enhances the discriminator's ability to enforce accurate image-text alignment during image generation, ultimately improving the StackGAN model's ability to generate images that accurately reflect the given textual descriptions.\\n\\nKeywords: mismatched images, StackGAN, discriminator, training, image-text alignment, text embeddings, conditional GANs, training diversity, negative examples, correct and incorrect matches. \\n\")],\n",
    " [Document(metadata={}, page_content=\"File_path: Papers/Mask RCNN.pdf \\n## Summary:\\n\\nMask R-CNN is a state-of-the-art deep learning framework for object instance segmentation, built upon Faster R-CNN. It introduces a parallel branch for predicting object masks alongside bounding boxes, enabling efficient and accurate instance segmentation.  \\n\\nThe key innovation is RoIAlign, which addresses spatial misalignment issues, significantly improving mask prediction accuracy. Mask R-CNN decouples mask and class prediction, simplifying training and achieving superior results compared to traditional methods.\\n\\nExtensive experiments on the COCO dataset demonstrate Mask R-CNN's exceptional performance, surpassing existing methods in instance segmentation, object detection, and person keypoint detection. Its versatility extends to other instance-level tasks, such as human pose estimation.\\n\\nMask R-CNN's speed, efficiency, and flexibility make it a powerful baseline for various applications.\\n\\n## Keywords:\\n\\nInstance Segmentation, Mask R-CNN, Faster R-CNN, RoIAlign, Object Detection, COCO Dataset, Human Pose Estimation, Keypoint Detection, Multi-Task Learning, Deep Learning, Convolutional Neural Networks, Feature Pyramid Network (FPN), ResNet, ResNeXt, Cityscapes Dataset \\n\")],\n",
    " [Document(metadata={}, page_content=\"File_path: Papers/Variational Auto encoders.pdf \\n## Summary:\\n\\nAuto-Encoding Variational Bayes (AEVB) is a groundbreaking algorithm for approximate inference and learning in probabilistic models with continuous latent variables. It tackles the challenges of intractable posterior distributions and large datasets by employing the Stochastic Gradient Variational Bayes (SGVB) estimator, which utilizes a reparameterization trick for unbiased gradient computation. AEVB optimizes a recognition model that approximates the intractable posterior, allowing for efficient approximate posterior inference using ancestral sampling. This eliminates the need for expensive iterative inference methods like MCMC. AEVB also allows joint learning of the recognition model and generative model parameters, leading to improved optimization.\\n\\nThe paper emphasizes AEVB's connection to existing methods like the Wake-Sleep algorithm, while highlighting its advantage of optimizing a single objective function directly linked to the marginal likelihood. AEVB further reveals a connection between directed probabilistic models and auto-encoders, where the variational lower bound acts as a regularizer, encouraging the learning of meaningful representations.\\n\\nExperimental results demonstrate AEVB's superior performance compared to existing methods like Wake-Sleep and Monte Carlo EM. AEVB consistently outperforms Wake-Sleep in terms of convergence speed and lower bound optimization, and exhibits faster convergence than Monte Carlo EM, particularly for large datasets. The paper includes visualizations of the learned latent manifolds for MNIST and Frey Face datasets, highlighting AEVB's ability to capture complex data structures.\\n\\nFuture directions for AEVB include its application to deep generative architectures with deep neural networks, its extension to time-series models, and its adaptation for learning complicated noise distributions in supervised settings.\\n\\n## Keywords:\\n\\nAuto-Encoding Variational Bayes (AEVB), Stochastic Gradient Variational Bayes (SGVB), Variational Inference, Directed Probabilistic Models, Continuous Latent Variables, Intractable Posterior, Large Datasets, Recognition Model, Ancestral Sampling, Reparameterization Trick, Auto-encoders, Wake-Sleep Algorithm, Monte Carlo EM, Marginal Likelihood, Latent Manifold, Deep Neural Networks, Time-Series Models, Supervised Learning. \\n\")],\n",
    " [Document(metadata={}, page_content='File_path: Papers/StackGAN_original_paper.pdf \\n## Summary:\\n\\nStackGAN is a novel method for generating high-resolution, photo-realistic images from text descriptions. It utilizes a two-stage Generative Adversarial Network (GAN) architecture. The first stage (Stage-I GAN) generates a low-resolution sketch capturing the basic shape and colors, while the second stage (Stage-II GAN) refines this sketch into a high-resolution image with intricate details. The Conditioning Augmentation Technique plays a crucial role in enhancing image diversity and training stability. StackGAN outperforms existing methods in generating high-quality images, as demonstrated by its impressive performance on various benchmark datasets. The analysis of its components highlights the significance of the stacked structure and the Conditioning Augmentation Technique. Overall, StackGAN represents a major advancement in text-to-image synthesis, offering a robust and effective approach for generating photo-realistic images from text descriptions.\\n\\n## Keywords:\\n\\nStackGAN, Text-to-Image Synthesis, Generative Adversarial Networks (GAN), Stage-I GAN, Stage-II GAN, Conditioning Augmentation Technique, Photo-realistic Images, Text Description, Image Generation, Inception Score, Human Rankings, Benchmark Datasets, CUB, Oxford-102, MS COCO, GAN-INT-CLS, GAWWN \\n')],\n",
    " [Document(metadata={}, page_content='File_path: Papers/Fast-RCNN.pdf \\n## Summary:\\n\\nFast R-CNN revolutionizes object detection by introducing a single-stage training framework, significantly improving both speed and accuracy compared to previous methods like R-CNN and SPPnet. This framework jointly learns to classify object proposals and refine their spatial locations, eliminating the multi-stage pipeline and enabling faster training and testing. Key innovations include back-propagation through RoI pooling, allowing updates to all network layers, and hierarchical mini-batch sampling, facilitating shared computation and memory for faster training. The multi-task loss further enhances accuracy by jointly optimizing for classification and bounding-box regression. \\n\\nFast R-CNN achieves state-of-the-art performance on the PASCAL VOC dataset, demonstrating its effectiveness and speed advantages. The paper also investigates various design choices, highlighting the importance of sparse object proposals for high accuracy and enabling efficient evaluation of different proposal schemes. Preliminary results on the MS COCO dataset establish a baseline for future research.\\n\\n\\n## Keywords: \\n\\nObject Detection, Deep Convolutional Networks, Fast R-CNN, R-CNN, SPPnet, Single-stage Training, Multi-task Loss, RoI Pooling, Back-propagation, Hierarchical Mini-batch Sampling, Scale Invariance, Object Proposals, PASCAL VOC, MS COCO, Average Precision (mAP), Training Speed, Testing Speed, Accuracy, Design Evaluation, Performance Comparison, Implementation Details, Computational Efficiency, Data Augmentation, Image Classification, Feature Extraction, Bounding Box Regression, Softmax, SVM, Average Recall (AR), Cascade, Dense Proposals, Sparse Proposals, Deep Learning, Computer Vision \\n')],\n",
    " [Document(metadata={}, page_content=\"File_path: Papers/GANs_Paper.pdf \\n## Summary:\\n\\nThis paper introduces Generative Adversarial Nets (GANs), a novel framework for estimating generative models. GANs train two models simultaneously: a generative model (G) that learns the data distribution and a discriminative model (D) that distinguishes real data from G's output.  G's training goal is to fool D, leading to a minimax game where G acts as a counterfeiter and D as the police.  Through competition, both models improve until generated data becomes indistinguishable from real data.\\n\\nGANs employ multilayer perceptrons for both G and D, enabling training with backpropagation. This eliminates the need for Markov chains or approximate inference networks, simplifying the process.\\n\\nTheoretically, the minimax game achieves global optimality when the generative distribution matches the data distribution. The paper also proves convergence of the training algorithm when both models have sufficient capacity.\\n\\nExperiments on MNIST, Toronto Face Database, and CIFAR-10 datasets demonstrate GANs' potential. The generated samples show competitive quality compared to existing methods, highlighting the framework's promise.\\n\\nGANs offer advantages like no need for Markov chains or approximate inference, training solely through backpropagation, and the ability to incorporate diverse functions. However, they also have disadvantages such as no explicit representation of the generative distribution and the requirement for careful synchronization of D and G during training.\\n\\nThe paper concludes by outlining potential extensions, including conditional generative models, learned approximate inference, semi-supervised learning, and efficiency improvements.\\n\\n---\\n\\nKeywords: Generative Adversarial Nets (GANs), Generative Models, Discriminative Models, Minimax Game, Multilayer Perceptrons, Backpropagation, Theoretical Analysis, Global Optimality, Convergence, Experiments, MNIST, Toronto Face Database, CIFAR-10, Advantages, Disadvantages, Extensions, Conditional Generative Models, Learned Approximate Inference, Semi-Supervised Learning, Efficiency Improvements. \\n\")],\n",
    " [Document(metadata={}, page_content=\"File_path: Papers/DeepSORT.pdf \\n## Summary:\\n\\nDeep SORT is an advanced multiple object tracking algorithm designed to address the limitations of SORT, particularly the frequent identity switches caused by occlusions. Building upon SORT's foundation of Kalman filtering and the Hungarian algorithm for data association, Deep SORT incorporates appearance information to enhance tracking accuracy. This is achieved by using a convolutional neural network (CNN) trained on a person re-identification dataset to generate appearance descriptors for each detected bounding box. These descriptors are then compared using cosine distance to establish associations between tracks and detections, significantly reducing identity switches. The paper delves into the implementation details of Deep SORT, including track handling, state estimation, the assignment problem formulation, and the matching cascade. The matching cascade prioritizes recently observed tracks, mitigating the increased uncertainty in Kalman filter predictions during prolonged occlusions. Extensive evaluations on the MOT16 benchmark demonstrate Deep SORT's effectiveness in reducing identity switches while maintaining competitive performance in terms of MOTA, MOTP, and other metrics. The algorithm is also computationally efficient, achieving approximately 20 Hz on a modern GPU.\\n\\n## Keywords:\\n\\nMultiple Object Tracking, SORT, Deep Association Metric, Appearance Information, Convolutional Neural Network, Person Re-identification, Kalman Filtering, Hungarian Algorithm, Matching Cascade, MOT16 Benchmark, Identity Switches, Occlusions, Real-time Tracking, Deep SORT, CNN, Cosine Distance, Track Handling, State Estimation, Assignment Problem, Matching Cascade, MOTA, MOTP, Computational Efficiency. \\n\")],\n",
    " [Document(metadata={}, page_content='File_path: Papers/DCGAN-notes.pdf \\n## Summary:\\n\\nThis document delves into the architecture and training process of Deep Convolutional Generative Adversarial Networks (DCGANs), a powerful technique for generating realistic images. DCGANs consist of two key components: a generator and a discriminator. The generator, composed of transposed convolutional layers, batch normalization, and ReLU activations, generates images from random noise. The discriminator, a binary classifier, uses convolutional layers, batch normalization, Leaky ReLU activations, and a sigmoid output to distinguish between real and fake images. \\n\\nThe generator and discriminator undergo adversarial training, where the generator aims to fool the discriminator by generating images indistinguishable from real ones, while the discriminator strives to accurately classify real and generated images. This adversarial training process allows DCGANs to produce high-quality images. \\n\\n**Keywords:** DCGAN, Generative Adversarial Network, Generator, Discriminator, Transposed Convolution, Batch Normalization, ReLU, Tanh, Leaky ReLU, Sigmoid, Adversarial Training, Loss Function, Image Generation. \\n')],\n",
    " [Document(metadata={}, page_content=\"File_path: Papers/Mamba.pdf \\n## Summary:\\n\\nMamba is a revolutionary linear-time sequence modeling architecture that outperforms Transformers in various domains like language, audio, and genomics. Unlike Transformers, Mamba utilizes Selective State Space Models (SSMs) with a novel selection mechanism that allows it to selectively propagate or forget information based on the input, enabling it to handle discrete modalities like language. This mechanism is implemented through a hardware-aware algorithm, leading to significant speedups.\\n\\nMamba achieves impressive performance on diverse tasks, including language modeling, audio modeling, and genomics.  For instance, a 3B parameter Mamba language model surpasses Transformers of the same size and matches the performance of Transformers twice its size. Mamba also excels in synthetic tasks like selective copying and induction heads, demonstrating its ability to reason about data and extrapolate solutions to million-length sequences.\\n\\nMamba offers significant advantages over Transformers:\\n\\n* **Linear Scaling:** Mamba's computational cost and memory usage scale linearly with sequence length, enabling efficient training and inference on long sequences.\\n* **High Throughput:** Mamba's recurrent nature allows for faster inference than Transformers, achieving up to 5x higher throughput.\\n* **Improved Performance:** Mamba consistently outperforms Transformers and other subquadratic architectures in various tasks and domains.\\n* **Long Context:** Mamba effectively utilizes long context, achieving performance improvements on real data up to million-length sequences.\\n\\nMamba's linear scaling, high throughput, improved performance, and ability to handle long context make it a promising alternative to Transformers for a wide range of sequence modeling tasks. \\n\\n## Keywords:\\n\\nMamba, Selective State Space Model (SSM), Linear-time sequence modeling, Transformer, Attention, Selection mechanism, Hardware-aware algorithm, Language modeling, Audio modeling, Genomics, Scaling laws, Long context, Efficiency, Speed, Memory, Downstream tasks, Foundation models, Deep learning, Performance,  Throughput,  Sequence length,  Discrete modalities,  Real data,  Synthetic tasks,  Induction heads,  Selective copying. \\n\")],\n",
    " [Document(metadata={}, page_content='File_path: Papers/Word2Vec Paper.pdf \\nThis paper introduces two novel neural network architectures, Continuous Bag-of-Words (CBOW) and Continuous Skip-gram, for generating word embeddings. These models outperform existing methods like NNLM and RNNLM in terms of accuracy and efficiency. CBOW predicts a word based on its surrounding context, while Skip-gram predicts context words given the current word. The paper demonstrates that these models can learn high-quality word vectors from massive datasets, achieving state-of-the-art performance on various tasks assessing semantic and syntactic word similarities.  The authors highlight the potential of these models to improve NLP applications like machine translation, information retrieval, and question answering systems.\\n\\nKeywords: Word Embeddings, Word Representations, Continuous Bag-of-Words (CBOW), Continuous Skip-gram, Neural Network Language Model (NNLM), Recurrent Neural Network Language Model (RNNLM), Distributed Representations, Word Similarity, Semantic Similarity, Syntactic Similarity, NLP, Machine Translation, Information Retrieval, Question Answering, Large-Scale Distributed Framework, DistBelief, Microsoft Sentence Completion Challenge. \\n')]]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in summaries: \n",
    "    vector_store.add_documents(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3d17c644-6b33-430d-bc7d-37a0e5836144']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the attention mechanism\"\n",
    "results = vector_store.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='File_path: Papers/Attention_is_all_you_need.pdf \\n## Summary:\\n\\nThe Transformer is a revolutionary neural network architecture designed for sequence transduction tasks, particularly machine translation, surpassing traditional RNNs in both performance and efficiency. This groundbreaking model utilizes self-attention, eliminating the need for recurrent connections and convolutions, enabling faster training and parallel processing. Key features include an encoder-decoder structure, multi-head attention for capturing long-range dependencies, and positional encodings to preserve token order. The Transformer achieved state-of-the-art results on English-to-German and English-to-French translation tasks, outperforming existing models, including ensembles, in both translation quality and training speed. It also demonstrated competitive performance on English constituency parsing. Future research aims to extend the Transformer to other tasks, explore different modalities, and investigate local attention mechanisms for handling large inputs and outputs.\\n\\n## Keywords:\\n\\nTransformer, Neural Network, Sequence Transduction, Machine Translation, Self-Attention, Multi-Head Attention, Positional Encoding, Encoder-Decoder, RNN, Convolutional Layer, Parallelism, Computational Efficiency, Long-Range Dependencies, WMT 2014, English-to-German, English-to-French, Constituency Parsing, State-of-the-Art, BLEU Score, Training Speed \\n'),\n",
       " Document(metadata={}, page_content=\"File_path: Papers/DeepSORT.pdf \\n## Summary:\\n\\nDeep SORT is an advanced multiple object tracking algorithm designed to address the limitations of SORT, particularly the frequent identity switches caused by occlusions. Building upon SORT's foundation of Kalman filtering and the Hungarian algorithm for data association, Deep SORT incorporates appearance information to enhance tracking accuracy. This is achieved by using a convolutional neural network (CNN) trained on a person re-identification dataset to generate appearance descriptors for each detected bounding box. These descriptors are then compared using cosine distance to establish associations between tracks and detections, significantly reducing identity switches. The paper delves into the implementation details of Deep SORT, including track handling, state estimation, the assignment problem formulation, and the matching cascade. The matching cascade prioritizes recently observed tracks, mitigating the increased uncertainty in Kalman filter predictions during prolonged occlusions. Extensive evaluations on the MOT16 benchmark demonstrate Deep SORT's effectiveness in reducing identity switches while maintaining competitive performance in terms of MOTA, MOTP, and other metrics. The algorithm is also computationally efficient, achieving approximately 20 Hz on a modern GPU.\\n\\n## Keywords:\\n\\nMultiple Object Tracking, SORT, Deep Association Metric, Appearance Information, Convolutional Neural Network, Person Re-identification, Kalman Filtering, Hungarian Algorithm, Matching Cascade, MOT16 Benchmark, Identity Switches, Occlusions, Real-time Tracking, Deep SORT, CNN, Cosine Distance, Track Handling, State Estimation, Assignment Problem, Matching Cascade, MOTA, MOTP, Computational Efficiency. \\n\"),\n",
       " Document(metadata={}, page_content=\"File_path: Papers/Mamba.pdf \\n## Summary:\\n\\nMamba is a revolutionary linear-time sequence modeling architecture that outperforms Transformers in various domains like language, audio, and genomics. Unlike Transformers, Mamba utilizes Selective State Space Models (SSMs) with a novel selection mechanism that allows it to selectively propagate or forget information based on the input, enabling it to handle discrete modalities like language. This mechanism is implemented through a hardware-aware algorithm, leading to significant speedups.\\n\\nMamba achieves impressive performance on diverse tasks, including language modeling, audio modeling, and genomics.  For instance, a 3B parameter Mamba language model surpasses Transformers of the same size and matches the performance of Transformers twice its size. Mamba also excels in synthetic tasks like selective copying and induction heads, demonstrating its ability to reason about data and extrapolate solutions to million-length sequences.\\n\\nMamba offers significant advantages over Transformers:\\n\\n* **Linear Scaling:** Mamba's computational cost and memory usage scale linearly with sequence length, enabling efficient training and inference on long sequences.\\n* **High Throughput:** Mamba's recurrent nature allows for faster inference than Transformers, achieving up to 5x higher throughput.\\n* **Improved Performance:** Mamba consistently outperforms Transformers and other subquadratic architectures in various tasks and domains.\\n* **Long Context:** Mamba effectively utilizes long context, achieving performance improvements on real data up to million-length sequences.\\n\\nMamba's linear scaling, high throughput, improved performance, and ability to handle long context make it a promising alternative to Transformers for a wide range of sequence modeling tasks. \\n\\n## Keywords:\\n\\nMamba, Selective State Space Model (SSM), Linear-time sequence modeling, Transformer, Attention, Selection mechanism, Hardware-aware algorithm, Language modeling, Audio modeling, Genomics, Scaling laws, Long context, Efficiency, Speed, Memory, Downstream tasks, Foundation models, Deep learning, Performance,  Throughput,  Sequence length,  Discrete modalities,  Real data,  Synthetic tasks,  Induction heads,  Selective copying. \\n\"),\n",
       " Document(metadata={}, page_content='File_path: Papers/DCGAN-notes.pdf \\n## Summary:\\n\\nThis document delves into the architecture and training process of Deep Convolutional Generative Adversarial Networks (DCGANs), a powerful technique for generating realistic images. DCGANs consist of two key components: a generator and a discriminator. The generator, composed of transposed convolutional layers, batch normalization, and ReLU activations, generates images from random noise. The discriminator, a binary classifier, uses convolutional layers, batch normalization, Leaky ReLU activations, and a sigmoid output to distinguish between real and fake images. \\n\\nThe generator and discriminator undergo adversarial training, where the generator aims to fool the discriminator by generating images indistinguishable from real ones, while the discriminator strives to accurately classify real and generated images. This adversarial training process allows DCGANs to produce high-quality images. \\n\\n**Keywords:** DCGAN, Generative Adversarial Network, Generator, Discriminator, Transposed Convolution, Batch Normalization, ReLU, Tanh, Leaky ReLU, Sigmoid, Adversarial Training, Loss Function, Image Generation. \\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_prompt=\"\"\"\n",
    "Using the Context bellow answer the question {question}, mention the path of the MOST RELAVENT documents.\n",
    "\n",
    "NOTE: The contexts are summaries of maybe very large documents, so scrutinize it well and at the end of each summary keywords are also mentioned, use these also to answer the question.RETURN ONLY PATHS TO THE MOST RELEVANT DOCUMENTS,multiple paths can be returned.\n",
    "Context:\n",
    "{text}\n",
    "\n",
    "IMPORTANT: The answer should be in the following format:\n",
    "RETURN ONLY JSON DATA NOTHING ELSE\n",
    "```\n",
    "    {{\n",
    "    \"files\": [\n",
    "        {{\n",
    "        \"file_path\": \"path to the file \",\n",
    "        \"relevance\": \"Why is this file relavaent\"\n",
    "        }}\n",
    "    ]\n",
    "    }}\n",
    "    ```\n",
    "\"\"\"\n",
    "RAG_prompt_template=PromptTemplate(input_variables=['text','question'],\n",
    "                                    template=chunks_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "GEMINI_API_KEY=os.environ.get(\"GEMINI_API\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Explain me what is the attention mechanism in 100 words\"\n",
    "# context = retriever.invoke(question)\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"text\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | RAG_prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content=\"File_path: Papers/Mamba.pdf \\n## Summary:\\n\\nMamba is a revolutionary linear-time sequence modeling architecture that outperforms Transformers in various domains like language, audio, and genomics. Unlike Transformers, Mamba utilizes Selective State Space Models (SSMs) with a novel selection mechanism that allows it to selectively propagate or forget information based on the input, enabling it to handle discrete modalities like language. This mechanism is implemented through a hardware-aware algorithm, leading to significant speedups.\\n\\nMamba achieves impressive performance on diverse tasks, including language modeling, audio modeling, and genomics.  For instance, a 3B parameter Mamba language model surpasses Transformers of the same size and matches the performance of Transformers twice its size. Mamba also excels in synthetic tasks like selective copying and induction heads, demonstrating its ability to reason about data and extrapolate solutions to million-length sequences.\\n\\nMamba offers significant advantages over Transformers:\\n\\n* **Linear Scaling:** Mamba's computational cost and memory usage scale linearly with sequence length, enabling efficient training and inference on long sequences.\\n* **High Throughput:** Mamba's recurrent nature allows for faster inference than Transformers, achieving up to 5x higher throughput.\\n* **Improved Performance:** Mamba consistently outperforms Transformers and other subquadratic architectures in various tasks and domains.\\n* **Long Context:** Mamba effectively utilizes long context, achieving performance improvements on real data up to million-length sequences.\\n\\nMamba's linear scaling, high throughput, improved performance, and ability to handle long context make it a promising alternative to Transformers for a wide range of sequence modeling tasks. \\n\\n## Keywords:\\n\\nMamba, Selective State Space Model (SSM), Linear-time sequence modeling, Transformer, Attention, Selection mechanism, Hardware-aware algorithm, Language modeling, Audio modeling, Genomics, Scaling laws, Long context, Efficiency, Speed, Memory, Downstream tasks, Foundation models, Deep learning, Performance,  Throughput,  Sequence length,  Discrete modalities,  Real data,  Synthetic tasks,  Induction heads,  Selective copying. \\n\"),\n",
       " Document(metadata={}, page_content=\"File_path: Papers/Variational Auto encoders.pdf \\n## Summary:\\n\\nAuto-Encoding Variational Bayes (AEVB) is a groundbreaking algorithm for approximate inference and learning in probabilistic models with continuous latent variables. It tackles the challenges of intractable posterior distributions and large datasets by employing the Stochastic Gradient Variational Bayes (SGVB) estimator, which utilizes a reparameterization trick for unbiased gradient computation. AEVB optimizes a recognition model that approximates the intractable posterior, allowing for efficient approximate posterior inference using ancestral sampling. This eliminates the need for expensive iterative inference methods like MCMC. AEVB also allows joint learning of the recognition model and generative model parameters, leading to improved optimization.\\n\\nThe paper emphasizes AEVB's connection to existing methods like the Wake-Sleep algorithm, while highlighting its advantage of optimizing a single objective function directly linked to the marginal likelihood. AEVB further reveals a connection between directed probabilistic models and auto-encoders, where the variational lower bound acts as a regularizer, encouraging the learning of meaningful representations.\\n\\nExperimental results demonstrate AEVB's superior performance compared to existing methods like Wake-Sleep and Monte Carlo EM. AEVB consistently outperforms Wake-Sleep in terms of convergence speed and lower bound optimization, and exhibits faster convergence than Monte Carlo EM, particularly for large datasets. The paper includes visualizations of the learned latent manifolds for MNIST and Frey Face datasets, highlighting AEVB's ability to capture complex data structures.\\n\\nFuture directions for AEVB include its application to deep generative architectures with deep neural networks, its extension to time-series models, and its adaptation for learning complicated noise distributions in supervised settings.\\n\\n## Keywords:\\n\\nAuto-Encoding Variational Bayes (AEVB), Stochastic Gradient Variational Bayes (SGVB), Variational Inference, Directed Probabilistic Models, Continuous Latent Variables, Intractable Posterior, Large Datasets, Recognition Model, Ancestral Sampling, Reparameterization Trick, Auto-encoders, Wake-Sleep Algorithm, Monte Carlo EM, Marginal Likelihood, Latent Manifold, Deep Neural Networks, Time-Series Models, Supervised Learning. \\n\"),\n",
       " Document(metadata={}, page_content='File_path: Papers/Word2Vec Paper.pdf \\nThis paper introduces two novel neural network architectures, Continuous Bag-of-Words (CBOW) and Continuous Skip-gram, for generating word embeddings. These models outperform existing methods like NNLM and RNNLM in terms of accuracy and efficiency. CBOW predicts a word based on its surrounding context, while Skip-gram predicts context words given the current word. The paper demonstrates that these models can learn high-quality word vectors from massive datasets, achieving state-of-the-art performance on various tasks assessing semantic and syntactic word similarities.  The authors highlight the potential of these models to improve NLP applications like machine translation, information retrieval, and question answering systems.\\n\\nKeywords: Word Embeddings, Word Representations, Continuous Bag-of-Words (CBOW), Continuous Skip-gram, Neural Network Language Model (NNLM), Recurrent Neural Network Language Model (RNNLM), Distributed Representations, Word Similarity, Semantic Similarity, Syntactic Similarity, NLP, Machine Translation, Information Retrieval, Question Answering, Large-Scale Distributed Framework, DistBelief, Microsoft Sentence Completion Challenge. \\n'),\n",
       " Document(metadata={}, page_content='File_path: Papers/Word2Vec Paper.pdf \\nThis paper introduces two novel neural network architectures, Continuous Bag-of-Words (CBOW) and Continuous Skip-gram, for generating word embeddings. These models outperform existing methods like NNLM and RNNLM in terms of accuracy and efficiency. CBOW predicts a word based on its surrounding context, while Skip-gram predicts context words given the current word. The paper demonstrates that these models can learn high-quality word vectors from massive datasets, achieving state-of-the-art performance on various tasks assessing semantic and syntactic word similarities.  The authors highlight the potential of these models to improve NLP applications like machine translation, information retrieval, and question answering systems.\\n\\nKeywords: Word Embeddings, Word Representations, Continuous Bag-of-Words (CBOW), Continuous Skip-gram, Neural Network Language Model (NNLM), Recurrent Neural Network Language Model (RNNLM), Distributed Representations, Word Similarity, Semantic Similarity, Syntactic Similarity, NLP, Machine Translation, Information Retrieval, Question Answering, Large-Scale Distributed Framework, DistBelief, Microsoft Sentence Completion Challenge. \\n')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"mamba\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\"files\": [\\n{\\n\"file_path\": \"Papers/Mamba.pdf\",\\n\"relevance\": \"This paper explicitly discusses Mamba as a potential successor to Transformers, highlighting its advantages in various domains and tasks.\"\\n}\\n]\\n}\\n```'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"which model is considered the next big thing after transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\"files\": [\\n{\\n\"file_path\": \"Papers/DCGAN-notes.pdf\",\\n\"relevance\": \"This document focuses specifically on the architecture and training process of Deep Convolutional Generative Adversarial Networks (DCGANs), which are a type of GAN. It provides details on the generator and discriminator components and their adversarial training process, making it highly relevant to understanding different types of GANs.\"\\n},\\n{\\n\"file_path\": \"Papers/StackGAN.pdf\",\\n\"relevance\": \"This document describes StackGAN, another type of GAN specifically designed for generating high-resolution images from textual descriptions. It highlights the two-stage process and the use of text encoding, making it relevant to understanding different GAN variants and their applications.\"\\n},\\n{\\n\"file_path\": \"Papers/StackGAN_original_paper.pdf\",\\n\"relevance\": \"This document further elaborates on StackGAN, providing deeper insights into its architecture and the Conditioning Augmentation Technique used to enhance image diversity. It discusses the performance of StackGAN on benchmark datasets, demonstrating its effectiveness in generating high-quality images. This information further contributes to understanding the variations in GANs and their capabilities.\"\\n},\\n{\\n\"file_path\": \"Papers/GANs_Paper.pdf\",\\n\"relevance\": \"While this paper introduces the foundational concept of Generative Adversarial Networks (GANs) in general, it does not delve into specific types of GANs like DCGANs or StackGANs. It serves as a foundation for understanding the core principles of GANs but lacks the specific details on different variations.\"\\n}\\n]\\n}\\n```'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What are the types of GANs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GlobalPythonEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
